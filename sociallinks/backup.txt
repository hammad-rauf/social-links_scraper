from scrapy import Request
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

import xlrd

from datetime import date

from ..items import SociallinksItem

class SocialLinkSpider(CrawlSpider):

    name = "sociallinks"

    socials = ["facebook","whatsapp","linkedin","instagram","twitter","skype","pinterest","telegram","wechat","qq","tumblr","reddit","xing","flickr","youtube"]

    rows = 0

    ignore = []

    def __init__(self, config_file = None, *args, **kwargs):                    
        super(SocialLinkSpider, self).__init__(*args, **kwargs)   

        loc = (config_file) 
    
        wb = xlrd.open_workbook(loc) 
        sheet = wb.sheet_by_index(0) 

        cols = sheet.ncols
        self.rows = sheet.nrows           

        self._config = []

        for row in range(1,self.rows): 

            link = sheet.cell_value(row,4)

            if len(link)!= 0:
                self._config.append(link) 
            else:
                self.ignore.append(row)

        self._url_list = self._config                                                              

    def start_requests(self):    

        for url in self._url_list:                                              
            yield Request(url = url, callback = self.parse)
   
    def parse(self, response):

        links = SociallinksItem()

        links["site"] = response.url    
        for social in self.socials:
            links[social] = response.css(f"a[href*='{social}']::attr(href)").extract_first()


        yield links

